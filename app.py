import streamlit as st
import pandas as pd
import scrapy
import json
import os

# --- Scrapy Spider for Price Scraping ---
class ProductPriceSpider(scrapy.Spider):
    name = "product_prices"
    
    # URLs to scrape for Flipkart, Amazon India, etc.
    start_urls = [
        "https://www.flipkart.com/search?q={product_name}",
        "https://www.amazon.in/s?k={product_name}",
        "https://www.croma.com/search/{product_name}",
        "https://www.reliancedigital.in/search?q={product_name}",
        "https://www.tatacliq.com/search/?search={product_name}"
    ]
    
    def start_requests(self):
        # Modify the start URLs with the search query for each product
        product_names = ["iPhone 13", "Samsung Galaxy"]
        for product in product_names:
            for url in self.start_urls:
                yield scrapy.Request(url.format(product_name=product), callback=self.parse)

    def parse(self, response):
        product_name = response.url.split("=")[-1]
        
        # Scraping logic for each site
        if "flipkart" in response.url:
            price = response.css('._30jeq3::text').get()
            site = "Flipkart"
        elif "amazon" in response.url:
            price = response.css('.a-price-whole::text').get()
            site = "Amazon India"
        elif "croma" in response.url:
            price = response.css('.price::text').get()
            site = "Croma"
        elif "reliancedigital" in response.url:
            price = response.css('.price-tag::text').get()
            site = "Reliance Digital"
        elif "tatacliq" in response.url:
            price = response.css('.product-price span::text').get()
            site = "Tata CLiQ"
        
        # Yielding the scraped data
        yield {
            'Product': product_name,
            'Site': site,
            'Price': price
        }

# --- Streamlit App for Displaying Results ---
def load_scraped_data():
    """Load the scraped data (prices.json generated by Scrapy)"""
    with open('prices.json') as f:
        return json.load(f)

# --- UI ---
st.set_page_config(page_title="üõçÔ∏è Price Comparison App", layout="wide")
st.title("üõçÔ∏è Product Price Comparison")

# Upload file
uploaded_file = st.file_uploader("üìÑ Upload product list", type=["csv", "txt"])

def parse_file(file):
    """Parse the uploaded file and return the list of product names"""
    if file.name.endswith(".csv"):
        df = pd.read_csv(file)
        return df.iloc[:, 0].dropna().tolist()
    elif file.name.endswith(".txt"):
        content = file.read().decode("utf-8").splitlines()
        return [line.strip() for line in content if line.strip()]
    return []

if uploaded_file:
    # Parse the uploaded product list
    products = parse_file(uploaded_file)

    if products:
        # Fetch the prices (we will simulate this with the data from the JSON file)
        scraped_data = load_scraped_data()
        
        # Filter data for the selected products
        filtered_data = [item for item in scraped_data if item['Product'] in products]

        # Create a DataFrame
        df = pd.DataFrame(filtered_data)

        if not df.empty:
            st.success("‚úÖ Price comparison complete!")
            
            # Group by product name and display price and site for each
            grouped_df = df.groupby('Product').apply(lambda x: x[['Site', 'Price']].to_string(index=False))
            st.write(grouped_df)

            # Add a column for the lowest price & site
            def get_lowest_price(group):
                prices = group['Price'].str.extract('(\d+[\.,]?\d*)').astype(float)
                group['Price'] = prices[0]
                min_row = group.loc[group['Price'].idxmin()]
                return f"{min_row['Site']} ({min_row['Price']})"

            # Display the lowest price and site in a separate column
            df['Lowest Price & Site'] = df.groupby('Product').apply(get_lowest_price).reset_index(level=0, drop=True)

            # Final result with lowest price & site
            st.write(df[['Product', 'Site', 'Price', 'Lowest Price & Site']])

            # Option to download the data
            csv = df.to_csv(index=False)
            st.download_button("üì• Download CSV", data=csv, file_name="price_comparison.csv", mime="text/csv")
        else:
            st.warning("‚ùå No results found for any products.")
    else:
        st.warning("‚ùå The uploaded file does not contain any valid products.")
